---
title: "Part 1: Introduction to LLMs"
order: 1
description: >
  Exploring what makes up Large Language Models (LLMs), from training neural
  networks to downloading and deploying them.
image: https://placehold.co/1600x900/f7f7f7/121212.png?text=Introduction+to+LLMs&font=playfair-display
author: Victor Roussekov
date: 2023-09-19
date-modified: last-modified
categories: ["LLMs", "Exploring LLMs", "Notebook"]
citation: true

share:
  permalink: https://victorouse.zip/blog/exploring-llms/1-introduction-to-llms
  description: What he said ðŸ‘‰
  twitter: true
  reddit: true
  linkedin: true
  email: true
  mastodon: true

jupyter: python3

---

---

### What are Large Language Models?

Large Language Models (LLMs) are neural networks that can generate output such
as text, images, etc. Each model has its own applied methodology, reflected in
how the neural network is configured.

To use neural networks, they must first be "trained". The training process
involves passing "training" data through the network and evaluating the output
against a "validation" or "testing" set.

This process adjusts the model "weights" along the way, and once the training
process is finished, the resulting weights can be distributed for use.

The resulting weights are generally referred to as "pre-trained" models that
are distributed in a format that can be "loaded" into a model.

There are many different types of models, from OpenAI's proprietary GPT models
(GPT3, GTP3.5, GTP4), to open source models like Meta's
[Llama2](https://ai.meta.com/llama/).

There are many open sources models that can be downloaded from a model artefact
repository called [HuggingFace](https://huggingface.co/). You can think of
HuggingFace like the DockerHub for LLMs.

There are also platforms that offer LLM hosting services, such as
[Replicate](https://replicate.com/), which host these LLM models on the cloud
for you, meaning you don't have to run them on your own hardware.

It is still possible to run LLMs on consumer hardware, and a MacBook should be
able to at least run small-to-medium sized models within reasonable timeframes.
